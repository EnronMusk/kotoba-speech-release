{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#For safe imports of everything\n",
    "notebook_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.insert(False, parent_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kotoba-speech-release/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dtype=bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kotoba-speech-release/.venv/lib/python3.10/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    }
   ],
   "source": [
    "from fam.llm.fast_inference import TTS\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter out user warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dtype=bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 42366.71it/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 37117.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 14.07M\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.1.0+cu121\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host futuristic-hand-glows-fin-01\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mGit commit: 65a024e, branch: release\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /root/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /root/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-04-26 07:59:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n",
      "/root/.cache/huggingface/hub/models--metavoiceio--metavoice-1B-v0.1/snapshots/d7237c5cb970da187623738f2957b338f3b0889a/first_stage.pt\n",
      "Using device=cuda\n",
      "Loading model ...\n",
      "using dtype=bfloat16\n",
      "Time to load model: 0.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:03<00:00, 55.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time: 4.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TTS(model_name='metavoiceio/metavoice-1B-v0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing the new synthesizer** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 320/1133 [00:05<00:14, 56.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 1st stage LLM inference: 5.70 sec total, 207.33 tokens/sec\n",
      "Bandwidth achieved: 517.68 GB/s\n",
      "Memory used: 13.44 GB\n",
      "\n",
      "WARNING: Number of tokens at each hierarchy must be of the same length!\n",
      "Truncating to min length of 590 tokens from 591 max.\n",
      "[591, 590]\n",
      "is_causal\n",
      "in_x  torch.Size([1, 2, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "non-causal batching: 100%|██████████| 1/1 [00:00<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx torch.Size([1, 2, 1024])\n",
      "audio input 6\n",
      "431\n",
      "probs 6\n",
      "probs torch.Size([1, 1024, 1025])\n",
      "\n",
      "WARNING: Number of tokens at each hierarchy must be of the same length!\n",
      "Truncating to min length of 590 tokens from 643 max.\n",
      "[590, 590, 643, 643, 643, 643, 643, 643]\n",
      "Text: The university of waterloo is a publicly funded research university located in ontario canada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-26 08:00:11\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[33m\u001b[1mAudio sampling rate does not match model sampling rate (24000, 48000). Resampling...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved audio to /root/kotoba-speech-release/fam/outputs/synth_The_university_of_waterlo_8ee32452-d618-42c1-8162-363b2c70fb41.wav\n",
      "\n",
      "Saved audio to /root/kotoba-speech-release/fam/outputs/synth_The_university_of_waterlo_8ee32452-d618-42c1-8162-363b2c70fb41.wav\n",
      "\n",
      "Total time to synth (s): 8.46227240562439\n",
      "Real-time factor: 1.08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/kotoba-speech-release/fam/outputs/synth_The_university_of_waterlo_8ee32452-d618-42c1-8162-363b2c70fb41.wav'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.synthesiseSequential(text=\"The university of waterloo is a publicly funded research university located in ontario canada.\", spk_ref_path=\"/root/kotoba-speech-release/fam/llm/waterloov2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 368/1782 [00:06<00:24, 56.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 1st stage LLM inference: 6.53 sec total, 94.08 tokens/sec\n",
      "Bandwidth achieved: 234.92 GB/s\n",
      "Memory used: 13.79 GB\n",
      "\n",
      "WARNING: Number of tokens at each hierarchy must be of the same length!\n",
      "Truncating to min length of 306 tokens from 307 max.\n",
      "[307, 306]\n",
      "is_causal\n",
      "in_x  torch.Size([1, 2, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "non-causal batching: 100%|██████████| 1/1 [00:00<00:00, 139.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx torch.Size([1, 2, 1024])\n",
      "audio input 6\n",
      "123\n",
      "probs 6\n",
      "probs torch.Size([1, 1024, 1025])\n",
      "\n",
      "WARNING: Number of tokens at each hierarchy must be of the same length!\n",
      "Truncating to min length of 306 tokens from 326 max.\n",
      "[306, 306, 326, 326, 326, 326, 326, 326]\n",
      "Text: I have a black cat and waterloo is cool.\n",
      "\n",
      "Saved audio to /root/kotoba-speech-release/fam/outputs/synth_I_have_a_black_cat_and_wa_c762b40f-a1a7-4f57-b7a2-28110c4f2f6e.wav\n",
      "\n",
      "Saved audio to /root/kotoba-speech-release/fam/outputs/synth_I_have_a_black_cat_and_wa_c762b40f-a1a7-4f57-b7a2-28110c4f2f6e.wav\n",
      "\n",
      "Total time to synth (s): 8.233639001846313\n",
      "Real-time factor: 2.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/kotoba-speech-release/fam/outputs/synth_I_have_a_black_cat_and_wa_c762b40f-a1a7-4f57-b7a2-28110c4f2f6e.wav'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.synthesiseSequential(text=\"I have a black cat and waterloo is cool.\", spk_ref_path=\"/root/kotoba-speech-release/fam/llm/black_cat.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing audio tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 25731.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/kotoba-speech-release/fam/llm/waterloostuffs.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m TrainedBPETokeniser(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_info)\n\u001b[1;32m     16\u001b[0m enc \u001b[38;5;241m=\u001b[39m EncodecDecoder(data_adapter_fn\u001b[38;5;241m=\u001b[39mdata_adapter_second_stage\u001b[38;5;241m.\u001b[39mdecode, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/kotoba-speech-release/fam/outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokeniser_decode_fn\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m---> 18\u001b[0m toks \u001b[38;5;241m=\u001b[39m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/kotoba-speech-release/fam/llm/waterloostuffs.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kotoba-speech-release/fam/llm/decoders.py:54\u001b[0m, in \u001b[0;36mEncodecDecoder.get_tokens\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mUtility method to get tokens from audio. Useful when you want to test reconstruction in some form (e.g.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mlimited codebook reconstruction or sampling from second stage model only).\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m wav, sr \u001b[38;5;241m=\u001b[39m \u001b[43maudio_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mbd_sample_rate:\n\u001b[1;32m     56\u001b[0m     wav \u001b[38;5;241m=\u001b[39m julius\u001b[38;5;241m.\u001b[39mresample_frac(wav, sr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mbd_sample_rate)\n",
      "File \u001b[0;32m~/kotoba-speech-release/.venv/lib/python3.10/site-packages/audiocraft/data/audio.py:140\u001b[0m, in \u001b[0;36maudio_read\u001b[0;34m(filepath, seek_time, duration, pad)\u001b[0m\n\u001b[1;32m    138\u001b[0m         wav \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(wav, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     wav, sr \u001b[38;5;241m=\u001b[39m \u001b[43m_av_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseek_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;129;01mand\u001b[39;00m duration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m     expected_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m*\u001b[39m sr)\n",
      "File \u001b[0;32m~/kotoba-speech-release/.venv/lib/python3.10/site-packages/audiocraft/data/audio.py:84\u001b[0m, in \u001b[0;36m_av_read\u001b[0;34m(filepath, seek_time, duration)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"FFMPEG-based audio file reading using PyAV bindings.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mSoundfile cannot read mp3 and av_read is more efficient than torchaudio.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    tuple of torch.Tensor, int: Tuple containing audio data and sample rate\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m _init_av()\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m af:\n\u001b[1;32m     85\u001b[0m     stream \u001b[38;5;241m=\u001b[39m af\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39maudio[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     86\u001b[0m     sr \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mcodec_context\u001b[38;5;241m.\u001b[39msample_rate\n",
      "File \u001b[0;32mav/container/core.pyx:420\u001b[0m, in \u001b[0;36mav.container.core.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/core.pyx:266\u001b[0m, in \u001b[0;36mav.container.core.Container.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/core.pyx:286\u001b[0m, in \u001b[0;36mav.container.core.Container.err_check\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/error.pyx:328\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/kotoba-speech-release/fam/llm/waterloostuffs.mp3'"
     ]
    }
   ],
   "source": [
    "from fam.llm.decoders import EncodecDecoder\n",
    "from fam.llm.inference import TiltedEncodec\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from fam.quantiser.text.tokenise import TrainedBPETokeniser\n",
    "\n",
    "data_adapter_second_stage = TiltedEncodec(end_of_audio_token=1024)\n",
    "_other_model_dir = snapshot_download(repo_id=\"metavoiceio/metavoice-1B-v0.1\")\n",
    "checkpoint_path = Path(f\"{_other_model_dir}/first_stage.pt\")\n",
    "\n",
    "checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=False)\n",
    "tokenizer_info = checkpoint.get(\"meta\", {}).get(\"tokenizer\", {})\n",
    "tokenizer = TrainedBPETokeniser(**tokenizer_info)\n",
    "\n",
    "enc = EncodecDecoder(data_adapter_fn=data_adapter_second_stage.decode, output_dir=\"/root/kotoba-speech-release/fam/outputs\", tokeniser_decode_fn=tokenizer)\n",
    "\n",
    "toks = enc.get_tokens(audio_path=\"/root/kotoba-speech-release/fam/llm/waterloostuffs.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2453, 2170, 2517, 2347, 2404, 2365, 2482, 2561]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hey there goose man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toks[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
